{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Project -  Stock Market Prediction</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 1: Pre-processing &  data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn.datasets as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDirectoryPath= \"C:\\\\Users\\\\nogahm\\\\PycharmProjects\\\\Project-Stock-Market-Prediction\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords=(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that clean the text\n",
    "def cleanText(text):\n",
    "    text = str(text)\n",
    "    if text.startswith(\"b\\\"\") or text.startswith(\"b\\'\"):\n",
    "        text = text[1:]\n",
    "    text = text.replace(\"\\\"\", \"\").replace(\"\\'\", \"\")\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # tokenize\n",
    "    filtered_sentence = word_tokenize(text)\n",
    "    # # remove stop words\n",
    "    filtered_sentence = [w for w in filtered_sentence if not w in stopWords]\n",
    "    filtered_sentence = [w for w in filtered_sentence if len(w)>1]\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(filtered_sentence)-1):\n",
    "        try:\n",
    "            filtered_sentence[i] = ps.stem(filtered_sentence[i])\n",
    "        except Exception as inst:\n",
    "            filtered_sentence[i]=filtered_sentence[i]\n",
    "    # remove stop words after stem\n",
    "    filtered_sentence = [w for w in filtered_sentence if not w in stopWords]\n",
    "    filtered_sentence = [w for w in filtered_sentence if len(w)>1]\n",
    "    text= \" \".join(filtered_sentence)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into data frame\n",
    "combinedDataDF = pd.read_csv( DataDirectoryPath+\"\\\\Combined_News_DJIA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text (stopWords, tokenize, stemming)\n",
    "headlines = combinedDataDF.columns[range(2, 27)]\n",
    "combinedDataDF[headlines] = combinedDataDF[headlines].applymap(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>number of Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>number of Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDist = []\n",
    "for c in [0, 1]:\n",
    "    temp=[c, combinedDataDF[\"Label\"].value_counts()[c]]\n",
    "    classDist.append(temp)\n",
    "pd.DataFrame(classDist, columns=['Class', 'number of Samples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms frequency - for each class, number of appearances of term:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Class', 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word Frequency\nsay               1560\nus                1365\nnew               1166\nworld             1153\nkill              1145\nyear              1005\nchina              989\nisrael             970\ngovern             937\npolic              849\npeopl              848\nrussia             791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Class', 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word Frequency\nsay                1809\nus                 1544\nkill               1375\nworld              1260\nnew                1254\nchina              1194\nisrael             1161\nyear               1142\ngovern             1071\npolic               995\nrussia              939\nprotest             909\nTerms frequency - for each class, number of records with term:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>term 1</th>\n",
       "      <th>term 2</th>\n",
       "      <th>term 3</th>\n",
       "      <th>term 4</th>\n",
       "      <th>term 5</th>\n",
       "      <th>term 6</th>\n",
       "      <th>term 7</th>\n",
       "      <th>term 8</th>\n",
       "      <th>term 9</th>\n",
       "      <th>term 10</th>\n",
       "      <th>term 11</th>\n",
       "      <th>term 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(say, 748)</td>\n",
       "      <td>(us, 680)</td>\n",
       "      <td>(new, 650)</td>\n",
       "      <td>(world, 638)</td>\n",
       "      <td>(kill, 630)</td>\n",
       "      <td>(year, 583)</td>\n",
       "      <td>(govern, 572)</td>\n",
       "      <td>(china, 561)</td>\n",
       "      <td>(peopl, 549)</td>\n",
       "      <td>(report, 497)</td>\n",
       "      <td>(polic, 492)</td>\n",
       "      <td>(countri, 490)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(say, 836)</td>\n",
       "      <td>(us, 759)</td>\n",
       "      <td>(kill, 735)</td>\n",
       "      <td>(world, 706)</td>\n",
       "      <td>(new, 701)</td>\n",
       "      <td>(year, 676)</td>\n",
       "      <td>(china, 656)</td>\n",
       "      <td>(govern, 644)</td>\n",
       "      <td>(polic, 596)</td>\n",
       "      <td>(israel, 591)</td>\n",
       "      <td>(state, 591)</td>\n",
       "      <td>(peopl, 581)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>term 1</th>\n",
       "      <th>term 2</th>\n",
       "      <th>term 3</th>\n",
       "      <th>term 4</th>\n",
       "      <th>term 5</th>\n",
       "      <th>term 6</th>\n",
       "      <th>term 7</th>\n",
       "      <th>term 8</th>\n",
       "      <th>term 9</th>\n",
       "      <th>term 10</th>\n",
       "      <th>term 11</th>\n",
       "      <th>term 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(say, 748)</td>\n",
       "      <td>(us, 680)</td>\n",
       "      <td>(new, 650)</td>\n",
       "      <td>(world, 638)</td>\n",
       "      <td>(kill, 630)</td>\n",
       "      <td>(year, 583)</td>\n",
       "      <td>(govern, 572)</td>\n",
       "      <td>(china, 561)</td>\n",
       "      <td>(peopl, 549)</td>\n",
       "      <td>(report, 497)</td>\n",
       "      <td>(polic, 492)</td>\n",
       "      <td>(countri, 490)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(say, 836)</td>\n",
       "      <td>(us, 759)</td>\n",
       "      <td>(kill, 735)</td>\n",
       "      <td>(world, 706)</td>\n",
       "      <td>(new, 701)</td>\n",
       "      <td>(year, 676)</td>\n",
       "      <td>(china, 656)</td>\n",
       "      <td>(govern, 644)</td>\n",
       "      <td>(polic, 596)</td>\n",
       "      <td>(israel, 591)</td>\n",
       "      <td>(state, 591)</td>\n",
       "      <td>(peopl, 581)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 15 most common words for each class\n",
    "termsFreqPerClass=[]\n",
    "print (\"Terms frequency - for each class, number of appearances of term:\")\n",
    "for currClass in [0, 1]:\n",
    "    # aspect 1\n",
    "    class_df = combinedDataDF.loc[combinedDataDF[\"Label\"] == currClass]\n",
    "    tempDF=pd.DataFrame()\n",
    "    tempDF['text']=class_df[class_df.columns[2:]].apply(\n",
    "        lambda x: \" \".join(x.dropna()),\n",
    "        axis=1\n",
    "    )\n",
    "    temp=[\" \".join(set(w.split(\" \"))) for w in tempDF['text']] #remove duplicates\n",
    "    temp=\" \".join(temp)\n",
    "    termFreq=pd.Series((temp).split()).value_counts()\n",
    "    termsFreqDF=pd.DataFrame()\n",
    "    termsFreqDF['Term']=termFreq.index\n",
    "    termsFreqDF['Freq']=list(termFreq)\n",
    "    setFreq=set(termsFreqDF['Freq'])\n",
    "    # get top 12\n",
    "    termFreq=termFreq[:12]\n",
    "    zipedFreq=zip(termFreq.index,termFreq)\n",
    "    zippedList=list(zipedFreq)\n",
    "    termsFreqPerClass.append([currClass]+zippedList)\n",
    "    # aspect 2\n",
    "    print (\"Class\",currClass)\n",
    "    word_frequency_per_class = pd.Series()\n",
    "    for headline_column in combinedDataDF[headlines]:\n",
    "        word_frequency_per_headline_column = pd.Series(\" \".join(class_df[headline_column]).split()).value_counts()\n",
    "        word_frequency_per_class = word_frequency_per_class.append(word_frequency_per_headline_column)\n",
    "    word_frequency_per_class = word_frequency_per_class.groupby(by=word_frequency_per_class.index).sum().sort_values(ascending=False)[:12]\n",
    "    print(word_frequency_per_class.to_frame('Word Frequency'))\n",
    "\n",
    "print (\"Terms frequency - for each class, number of records with term:\")\n",
    "pd.DataFrame(termsFreqPerClass,columns=['Class','term 1','term 2','term 3','term 4','term 5','term 6','term 7','term 8','term 9','term 10', 'term 11', 'term 12'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 2: machine learning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# x = combined_news_djia_df[combined_news_djia_df.columns[2:4]]\n",
    "x=combinedDataDF.drop(['Label'], axis=1)\n",
    "y = combinedDataDF[[\"Label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test (30%-Test, 70%-Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all text column, and convert to list\n",
    "xTrain = xTrain.apply(lambda x: \" \".join(x), axis=1)\n",
    "xTest = xTest.apply(lambda x: \" \".join(x), axis=1)\n",
    "xTrain=xTrain.tolist()\n",
    "xTest=xTest.tolist()\n",
    "yTrain=yTrain['Label'].tolist()\n",
    "yTest=yTest['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will use is SVM (not Deep Learning, aka Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "parameters = {'vect__max_df': (0.3,0.4,0.5,0.6,0.7,0.75, 1.0),'clf__alpha': (0.0001, 0.01,1.0)}\n",
    "#TF-IDF & SVM\n",
    "tfIdfVectorizer=TfidfVectorizer(sublinear_tf=True,stop_words='english')\n",
    "ipeline = Pipeline([('vect', tf_idf_vectorizer), ('clf', SGDClassifier() )])\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "gs_clf = gs_clf.fit(xTrain, yTrain)\n",
    "prediction = gs_clf.predict(xTest)\n",
    "accuracy = metrics.accuracy_score(yTest, prediction)\n",
    "params = gs_clf.best_params_\n",
    "results.append(['TF-IDF' , 'SVM', params,accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['TF-IDF', 'SVM', {'vect__max_df': 0.3, 'clf__alpha': 0.01}, 0.52931323283082077]]\nAUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print 'AUC:', metrics.roc_auc_score(yTest, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SVM with different pre-proccesing - remove five most common words (in order to check if accuracy/AUC improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWords={'say','us','new','kill','world'}\n",
    "xTrainRemoveCommon=[w for w in xTrain if not w in removeWords]\n",
    "xTestRemoveCommon=[w for w in xTest if not w in removeWords]\n",
    "yTrainRemoveCommon=[w for w in yTrain if not w in removeWords]\n",
    "yTestRemoveCommon=[w for w in yTest if not w in removeWords]\n",
    "\n",
    "resultsCommon=[]\n",
    "parametersCommon = {'vect__max_df': (0.3,0.4,0.5,0.6,0.7,0.75, 1.0),'clf__alpha': (0.0001, 0.01,1.0)}\n",
    "#TF-IDF & SVM\n",
    "tfIdfVectorizerCommon=TfidfVectorizer(sublinear_tf=True,stop_words='english')\n",
    "ipelineCommon = Pipeline([('vect', tf_idf_vectorizer), ('clf', SGDClassifier() )])\n",
    "gs_clfCommon = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "gs_clfCommon = gs_clf.fit(xTrain, yTrain)\n",
    "predictionCommon = gs_clf.predict(xTest)\n",
    "accuracyCommon = metrics.accuracy_score(yTest, prediction)\n",
    "paramsCommon = gs_clf.best_params_\n",
    "resultsCommon.append(['TF-IDF' , 'SVM', paramsCommon,accuracyCommon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['TF-IDF', 'SVM', {'vect__max_df': 0.3, 'clf__alpha': 0.01}, 0.52931323283082077]]\nAUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(resultsCommon)\n",
    "print 'AUC:', metrics.roc_auc_score(yTest, predictionCommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model we will use is Neural Network (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras.models",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-3f728985c3e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras.models"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bca682fa7227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# design network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#create the model\n",
    "top_words=10000\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "xTrain2=tfIdfVectorizer.fit_transform(xTrain)\n",
    "xTest2=tfIdfVectorizer.fit_transform(xTest)\n",
    "model.fit(xTrain2, yTrain, validation_data=(xTest2, yTest), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(xTest2, yTest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "y_pred_keras = model.predict(xTest2).ravel()\n",
    "print 'AUC:', metrics.roc_auc_score(yTest, y_pred_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 3: Crawl updated data for 10 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect to reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id='pwzaClgaAR7lpw',\n",
    "                     client_secret='90fJL78P0316g8IpW_0cGqfhmpM',\n",
    "                     password='maya1234',\n",
    "                     user_agent='danrave',\n",
    "                     username='drave23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get posts from the last month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts=[]\n",
    "for submission in reddit.subreddit('worldnews').top('month', limit=1000):\n",
    "    # print(submission.title)\n",
    "    new_posts.append(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtered the last 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = pd.DataFrame()\n",
    "titles=[]\n",
    "createDate=[]\n",
    "start = 1549285503\n",
    "end = 1550353401\n",
    "for post in new_posts:\n",
    "  if start < post.created_utc < end:  \n",
    "    titles.append(post.title.encode('ascii', 'ignore'))\n",
    "    createDate.append(datetime.datetime.fromtimestamp(post.created).strftime('%d/%m/%Y'))\n",
    "filtered['Title']=titles\n",
    "filtered['Date']=createDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get 25 news from each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsDF={}\n",
    "# print filtered\n",
    "for index, row in filtered.iterrows():\n",
    "    d= row['Date']\n",
    "    if d not in newsDF:\n",
    "         newsDF[d] = []\n",
    "    if not (len(newsDF[d]) >= 25):\n",
    "        newsDF[d].append(row['Title'])\n",
    "newsDF = pd.DataFrame.from_dict(newsDF, orient='index').sort_index()\n",
    "# print(newsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process data from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_columns = newsDF.columns[range(0, 25)]\n",
    "newsDF[headlines_columns] = newsDF[headlines_columns].applymap(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get label from Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahooPage = urllib.urlopen(\"https://finance.yahoo.com/quote/%5EDJI/history?p=%5EDJI\")\n",
    "soup = BeautifulSoup(yahooPage, \"html.parser\")\n",
    "djiaTable = pd.read_html(str(soup.find(\"table\", class_=\"W(100%) M(0)\")), header=0)[0]\n",
    "djiaTable = djiaTable.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate label by Yahoo information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "dates=[]\n",
    "for index, row in djiaTable.iterrows():\n",
    "    open=row['Open']\n",
    "    close=row['Close*']\n",
    "    # not increase\n",
    "    if(open>close):\n",
    "        labels.append(0)\n",
    "    # increase\n",
    "    else:\n",
    "        labels.append(1)\n",
    "    dates.append(datetime.datetime.strptime(row['Date'], '%b %d, %Y').strftime('%d/%m/%Y'))\n",
    "djiaLabeledDF=pd.DataFrame()\n",
    "djiaLabeledDF['Date']=dates\n",
    "djiaLabeledDF['Label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join news with labels by dates\n",
    "newsWithLabel = pd.merge(djiaLabeledDF[[\"Date\", \"Label\"]], newsDF, left_on=\"Date\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict 10 days by SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=newsWithLabel.drop(['Label','Date'], axis=1)\n",
    "y = newsWithLabel[[\"Label\"]]\n",
    "x=x.apply(lambda x: \" \".join(x), axis=1).tolist()\n",
    "prediction = gs_clf.predict(x)\n",
    "print \"Prediction using SVM:\"\n",
    "print(prediction)\n",
    "print \"Accuracy using SVM:\"\n",
    "print(metrics.accuracy_score(y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict 10 days by RNN-LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionRNN=model.predict(x)\n",
    "print \"Prediction using RNN:\"\n",
    "print(predictionRNN)\n",
    "print \"Accuracy using RNN:\"\n",
    "print(metrics.accuracy_score(y, predictionRNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
